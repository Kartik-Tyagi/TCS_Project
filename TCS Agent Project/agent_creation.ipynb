{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "3da5f7e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "from langchain_classic.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.llms import Ollama\n",
    "from langchain_core.output_parsers import JsonOutputParser\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import OllamaEmbeddings\n",
    "from langchain_classic.chains import RetrievalQA\n",
    "from langchain_classic.prompts import PromptTemplate\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e67c2633",
   "metadata": {},
   "outputs": [],
   "source": [
    "FILE_PATH = \"sample.pdf\"\n",
    "DOC_CHUNK_SIZE = 800\n",
    "DOC_CHUNK_OVERLAP = 150\n",
    "INGESTION_MODEL = \"qwen2.5:7b\"\n",
    "EMBEDDING_MODEL = \"nomic-embed-text\"\n",
    "VECTORDB_DIR = \"./chroma_store\"\n",
    "QUERY_MODEL = \"llama3.1:8b\"\n",
    "QUERY_TEMPERATURE = 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "057914b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_metadata(meta: dict) -> dict:\n",
    "    clean = {}\n",
    "    for key, value in meta.items():\n",
    "        if isinstance(value, list):\n",
    "            clean[key] = \", \".join(map(str, value))\n",
    "        elif isinstance(value, dict):\n",
    "            clean[key] = str(value)\n",
    "        else:\n",
    "            clean[key] = value\n",
    "    return clean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5aba1129",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File loaded\n",
      "Docs created\n",
      "Num:  4\n"
     ]
    }
   ],
   "source": [
    "loader = PyPDFLoader(FILE_PATH)\n",
    "docs = loader.load()\n",
    "\n",
    "print(\"File loaded\")\n",
    "\n",
    "splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=DOC_CHUNK_SIZE,\n",
    "    chunk_overlap=DOC_CHUNK_OVERLAP\n",
    ")\n",
    "docs = splitter.split_documents(docs)\n",
    "\n",
    "print(\"Docs created\")\n",
    "print(\"Num: \", len(docs))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "6adaae3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata_llm = Ollama(\n",
    "    model=INGESTION_MODEL,\n",
    "    temperature=0\n",
    ")\n",
    "\n",
    "prompt = PromptTemplate(\n",
    "    input_variables=[\"chunk\", \"prev\", \"next\"],\n",
    "    template=\"\"\"\n",
    "You are annotating a document chunk for retrieval.\n",
    "\n",
    "Chunk:\n",
    "{chunk}\n",
    "\n",
    "Previous:\n",
    "{prev}\n",
    "\n",
    "Next:\n",
    "{next}\n",
    "\n",
    "Return valid JSON with:\n",
    "summary\n",
    "chunk_type\n",
    "importance_score\n",
    "main_topics\n",
    "prev_relation\n",
    "next_relation\n",
    "\"\"\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "50c4af7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating metadata\n",
      "Working on doc 1\n",
      "Time taken: 94.0237090587616\n",
      "Working on doc 2\n",
      "Time taken: 99.41127634048462\n",
      "Working on doc 3\n",
      "Time taken: 89.39200353622437\n",
      "Working on doc 4\n",
      "Time taken: 65.14144039154053\n",
      "Metadata created\n"
     ]
    }
   ],
   "source": [
    "metadata_chain = prompt | metadata_llm | JsonOutputParser()\n",
    "\n",
    "print(\"Creating metadata\")\n",
    "\n",
    "for i, doc in enumerate(docs):\n",
    "    print(\"Working on doc\", i + 1)\n",
    "    start = time.time()\n",
    "    meta = metadata_chain.invoke({\n",
    "        \"chunk\": doc.page_content,\n",
    "        \"prev\": docs[i-1].page_content if i > 0 else \"None\",\n",
    "        \"next\": docs[i+1].page_content if i < len(docs)-1 else \"None\"\n",
    "    })\n",
    "\n",
    "    meta = normalize_metadata(meta)\n",
    "    meta[\"position\"] = i\n",
    "    doc.metadata.update(meta)\n",
    "\n",
    "    print(\"Time taken:\", time.time() - start)\n",
    "\n",
    "print(\"Metadata created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "589c7e31",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stored in vector DB\n",
      "Ingestion complete\n"
     ]
    }
   ],
   "source": [
    "embeddings = OllamaEmbeddings(\n",
    "    model=EMBEDDING_MODEL\n",
    ")\n",
    "\n",
    "vectordb = Chroma.from_documents(\n",
    "    documents=docs,\n",
    "    embedding=embeddings,\n",
    "    persist_directory=VECTORDB_DIR\n",
    ")\n",
    "\n",
    "print(\"Stored in vector DB\")\n",
    "\n",
    "vectordb.persist()\n",
    "print(\"Ingestion complete\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "06248d9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "qa_llm = Ollama(\n",
    "    model=QUERY_MODEL,\n",
    "    temperature=QUERY_TEMPERATURE\n",
    ")\n",
    "\n",
    "vectordb = Chroma(\n",
    "    persist_directory=VECTORDB_DIR,\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "retriever = vectordb.as_retriever(\n",
    "    search_kwargs={\n",
    "        \"k\": 6,\n",
    "        \"filter\": {\n",
    "            \"importance_score\": {\"$gte\": 0.3}\n",
    "        }\n",
    "    }\n",
    ")\n",
    "\n",
    "qa = RetrievalQA.from_chain_type(\n",
    "    llm=qa_llm,\n",
    "    retriever=retriever,\n",
    "    return_source_documents=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1ed7bd98",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\tyagi\\AppData\\Local\\Temp\\ipykernel_31180\\1976721598.py:3: LangChainDeprecationWarning: The method `Chain.__call__` was deprecated in langchain-classic 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
      "  response = qa(query)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kartik's main skills appear to be:\n",
      "\n",
      "1. Proficiency in Python\n",
      "2. Familiarity with PyTorch\n",
      "3. Experience with prompt engineering and LLM APIs (Large Language Model Application Programming Interfaces)\n",
      "4. Knowledge of AI, including Generative AI projects\n",
      "5. Certifications in:\n",
      "\t* IBM AI Developer\n",
      "\t* AWS AI Practitioner\n",
      "\t* AWS Cloud Practioner\n",
      "\t* Azure Certifications\n",
      "\n",
      "He also has experience building and evaluating machine learning models using CNNs (Convolutional Neural Networks) and RNNs (Recurrent Neural Networks).\n"
     ]
    }
   ],
   "source": [
    "query = \"What are Kartik's main skills?\"\n",
    "\n",
    "response = qa(query)\n",
    "print(response[\"result\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "04e2d6c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ollama is working.\n"
     ]
    }
   ],
   "source": [
    "llm = Ollama(model=\"llama3.1:8b\")\n",
    "print(llm(\"Say exactly: Ollama is working\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
